## 02. 데이터 인프라

### 데이터 인프라의 핵심 구성 요소

- 클라우드 데이터 웨어하우스 및 데이터 레이크
- 데이터 수집 도구
- 데이터 소스의 댜앙성
- 워크플로 오케스트레이션 플랫폼
- 모델링 도구 및 프레임워크

### 데이터 소스의 다양성

**소스 시스템 소유권**

분석 팀은 조직이 구축하고 소유한 소스 시스템과 타사 도구에서 데이터를 수집하는 것이 일반적이다. 

- 이커머스 회사는 고객 장바구니 데이터를 앱과 연결된 데이터베이스에 저장
- GA와 같은 타사 웹 분석 도구 사용

두 데이터 소스를 함께 사용하면 구매까지 이어지는 고객 행동을 파악할 수 있다. 따라서 이러한 고객 행동의 분석으로 끝나는 데이터 파이프라인은 두 소스 모두에서 데이털글 수집하면서 시작된다. 

해당 예시에서는 PostgresDB와 GA api에서 얻은 데이터를 AWS S3에 로드한 다음 AWS Redshift에 로드하는 파이프라인을 예시로 들었다. 

> 기존 프로젝트에서는 S3이 이미지 URL만 넣어서 자세한 건 몰랐는데 다양한 데이터를 S3 버킷에 넣을 수 있구나를 알게 되었다. 학습을 진행하면서 좀 더 확인을 해봐야겠다.   (**[EATceed](https://wwns1411.tistory.com/12)프로젝트에서 로그 데이터 파이프라인을 구축해야하는데 도입을 할 수 있을 것 같다.**) 

소스 시스템이 위치하는 곳이 어디인지를 이해하는 것이 중요하다.

- 타사 데이터 소스에 위치한 데이터에 액세스하려고 한다면 제한이 있을 수 밖에 없다.
- 내부적으로 구축된 시스템은 액세스 방법뿐만 아니라 데이터를 사용자가 필요로 하는 형태에 맞게 맞추어 정의하는 등의 분석 팀에 많은 것을 제공할 수 있다.

주의해야 할 점은 시스템을 구축할 때 데이터 수집을 고려하여 설계했는지는 또 다른 문제다. 

- 데이터 수집이 시스템 의도하지 않은 부하를 가하는지
- 데이터를 점진적으로 로드할 수 있는지

**수집 인터페이스 및 데이터 구조**

데이터 엔지니어가 가장 먼저 알아봐야할 것은 다음과 같다.

- 소스 데이터가 어디에 있든지 새로운 데이터를 수집할 때 **소스 데이터를 얻는 방법과 형식**

각 인터페이스와 데이터 구조는 **각각의 도전 과제와 기회**를 동시에 가지고 있다. (Trade off)

- 잘 구성된 데이터는 작업하기 쉽지만, 일반적으로 앱/웹을 위해서 정형화
    - 분석을 하기위해서는 클렌징 및 변환 작업 등을 파이프라인에서 수행 필요
- 반정형 : JSON(Key, Value)
- 비정형 : AI 학습에 많이 사용

> 결국 어떻게 얻고 어떤 형식의 데이터인지에 따라 파이프라인 구축 방법을 다르게 가져가야 한다. 

**데이터 사이즈**

해당 내용을 읽었을 때 기존에 생각했던 것과 다른 점을 알게 되었다. 

대부분의 조직에서는 큰 데이터보다 작은 데이터세트를 더 중요하게 생각한다는 것이다. 또한, 크고 작은 데이터세트를 함께 수집하고 모델링하는 것이 일반적이라는 것이다. 즉, 파이프라인의 각 단계를 설계할 때 데이터 사이즈를 고려해야하지만 데이터 사이즈가 크다고 가치가 높은 것은 아니다.

데이터 파이프라인 구축 팁은 다음과 같다.

- 용량에 따라 파이프라인을 구축하는 것이 아닌 스펙트럼(규모) 측면에서 구축을 생각하자

**데이터 클렌징 작업과 유효성 검사**

지저분한 데이터의 공통적인 특징은 다음과 같다.

- 중복되거나 모호한 레코드
- 고립된 레코드(?) : 데이터셋 내에서 다른 데이터와 연결되거나 관계가 없는 독립적인 데이터 항목을 의미
- 누락된 레코드
- 텍스트 인코딩 오류(UTF-8 ..)
- 일치하지 않은 형식(대쉬(-)가 있거나 없는 전화번호)
- 레이브 지정이 잘 못 되거나 않은 데이터

작업 및 검사의 접근 방식은 다음과 같다.

- 최악을 가정하고 최상을 기대하라
    - 좋은 데이터는 학술에서만 있다. 깨끗한 출력을 위해 데이터를 식별하고 정리하는 파이프라인을 구축한다고 생각하자.
- 가장 적합한 시스템에서 데이터를 정리하고 검증하라
    - 파이프라인에서 나중에 데이터를 정리할 때 까지 기다리는 것이 좋을 때가 있다.
    - 결국 상황에 따라 ETL vs ELT 선택하라.
- 자주 검증하라
    - 데이터 검증을 파이프라인이 끝날 때까지 기다리지 않아야 한다.
    - 파이프라인 초기에 확인을 했어도 중간중간에 계속 확인을 해줘야한다.

> 해당 부분에 많은 공감이 간다. [ParkScore](https://wwns1411.tistory.com/13) 프로젝트를 진행할 때 데이터가 너무 지저분해서 클렌징 작업에서 많은 시간이 소요되었다. 위에서 언급해준 접근 방식을 토대로 마인드를 가지고 진행하도록 해야겠다.

### 클라우드 데이터 웨어하우스 및 데이터 레이크

**왜 클라우드 기반으로 수행하는 것일까?**

- 클라우드에서 데이터 파이프라인, 데이터 레이크, 웨어하우스 및 분석 처리 구축 및 배포가 쉬워졌다.
- 클라우드 공급업체에서 관리해주는 관리 서비스(특히 DB)가 주류
- 지속적인 클라우드 내 스토리지(S3) 비용 감소
- Redshift, Snowflake, BigQuery와 같은 확장성이 뛰어난 열 기반 데이터베이스 등장

**데이터 웨어하우스**

사용자가 원하는 질문에 대답할 수 있는 데이터 분석 활동을 지원하기 위해 서로 다른 시스템의 데이터가 모델링되어 저장되는 데이터베이스(분석 쿼리를 위해 정형화 및 최적화)

**데이터 레이크**

다양한 유형의 데이터와 대량의 데이터가 포함되어있다. 표준 데이터베이스처럼 정형화된 데이터를 쿼리하는 데 최적화되지는 않았다. 

### 데이터 변환 및 모델링 도구

**데이터 변환**

- ETL 또는 ELT 프로세스에서 T(transform)에 해당하는 광범위한 용어
- 시간대 변환과 같은 간단한 것에서부터 비즈니스 로직을 통한 집계 및 필터링

**데이터 모델링**

- 데이터 변환보다 구체적인 데이터 변환 유형
- 데이터 분석을 위해 데이터를 이해하고 최적화된 형식으로 정형화하고 정의

데이터 변환과 데이터 모델링 작업을 하기 위해서는 Python, SQL과 같은 언어로 수행한다. 

특히 SQL은 데이터 엔지니어에게 중요한 언어이다.(SQL을 배워야하는 이유)

### 워크플로 오케스트레이션 플랫폼

데이터 파이프라인의 복잡성과 수가 증가함에 따라 파이프라인에서 작업의 스케줄링 및 흐름을 관리해주는 워크플로 오케스트레이션 플랫폼을 도입하는 것이 중요하다. 

예를 들면 다음과 같다.

- 파이썬으로 작성된 데이터 수집 작업부터 하루 종일 특정 순서로 실행되어야 하는 SQL로 작성된 데이터 변환 작업에 이르기까지 12가지 작업을 수행하는 파이프라인

이런 경우 각 작업 간의 종속성을 예약하고 관리하는 것이 간단한 작업이 아니기 때문에 이때 사용하는 것이 바람직하다. 

- 일반적인 용도 : Apache Airflow, AWS Glue …
- 구체적인 사용 : Kubeflow Pipeline(Docker 컨테이너에 구축된 머신러닝 워크플로)

### 방향성 비순환 그래프

거의 모든 최신 오케스트레이션 프레임워크는 파이프라인에서 작업의 흐름과 종속성을 그래프로 나타낸다. 

파이프라인 그래프에는 몇 가지 특정 제약 조건이 있다.

- 항상 **방향성**을 가진다
    - 실행 경로와 순서를 보장
    - 모든 종속 작업이 완료되어야만 다음 작업 실행
- 비순환 그래프
    - 작업은 돌아갈 수 없기 때문에 비순환

위의 제약 조건을 고려해서 오케스트레이션 파이프라인은 방향성 비순환 그래프(DAG)를 생성한다. 

![img](https://github.com/user-attachments/assets/7d8fcf8d-036a-4720-9dfd-27197ac4d2a9)


- 그림 설명:
    - 네 가지 작업이 있는 DAG로 작업 a가 완료되면 작업 b,c가 실행되고 둘 다 완료되면 작업 d가 실행된다.

> Airflow에서는 DAG를 통해서 작업을 관리한다고 하였는데 **방향성 비순환 그래프** 내용을 읽고난 후 이해가 되었다!

### 데이터 인프라 커스터마이징

데이터 인프라가 정확히 동일한 조직은 찾아보기 어렵다.

이는 당연한 이야기로 요구 사항이 모두 다르기 때문에 공급업체를 선택할 수도 있고, 자체적으로 구축할 수 있기 때문이다. 

> **중요한 것은 제약 조건(비용, 엔지니어링 리소스, 보안 …)과 그에 따른 트레이드오프를 이해하는 것이다!**
